# Mashreq AI Command Center - System Documentation

## 1. System Overview

The **Mashreq AI Command Center** is a specialized "Responsible AI" system designed for the banking sector. Its primary purpose is to ingest, analyze, and triage high-velocity social media signals (e.g., tweets, posts) to detect operational risks such as:
- **Service Outages**: "App is down", "Can't login"
- **Fraud Patterns**: "OTP delay", "Phishing attempts", "Unauthorized withdrawals"
- **Misinformation**: "Liquidity rumors", "Bank run fake news"

Unlike standard sentiment analysis tools, this system is built on a **Strict 10-Stage Governance Pipeline**. This means no AI decision is made in a "black box"â€”every signal must pass through rigorous checks for PII (Personally Identifiable Information), synthetic data validation, and human-in-the-loop verification before actionable alerts are generated.

---

## 2. Dashboard Capabilities ( The "Website" )

The frontend is built with **Streamlit** using a premium "Glassmorphism" UI design. It serves as the cockpit for Risk Analysts.

### **Tab 1: Signal Triage (The "Inbox")**
*   **Purpose**: The primary workspace for analysts.
*   **What it shows**: A prioritized stream of incoming risk signals.
*   **Key Features**:
    *   **AI Reasoning Panel**: A dedicated box explaining *why* the AI flagged this code (e.g., "Signal: Potential fraud pattern... Why it matters: Regulatory fine risk").
    *   **Risk & Confidence Scores**: Visual badges (e.g., "Risk: 8/10", "Confidence: 94%").
    *   **Decision Buttons**: "Escalate" (move to next stage) or "Dismiss" (mark as noise).

### **Tab 2: Escalation Hub (The "Action Center")**
*   **Purpose**: managing active threats that have been validated.
*   **What it shows**: Only high-risk signals that require departmental action.
*   **Key Features**:
    *   **AI Action Plans**: Step-by-step distinct checklists generated by the AI (e.g., "1. Freeze incident... 2. Notify PR...").
    *   **Department Routing**: One-click routing to specific teams: IT Ops, Fraud Prevention, Comms, Risk Mgmt, CX, Compliance.

### **Tab 3: Audit Trail (The "Black Box Recorder")**
*   **Purpose**: Accountability and transparency.
*   **What it shows**: A chronological log of every human and AI interaction.
*   **Key Features**:
    *   **Executive Summary**: High-level counters of Escalations vs. Dismissals.
    *   **Management Report**: Generates a downloadable Markdown report for C-suite briefings.
    *   **Immutable Logs**: Timestamped entries for every button click.

### **Tab 4: Governance Center (The "Rulebook")**
*   **Purpose**: Explaining the AI's boundaries.
*   **What it shows**: Static documentation defining the system's limits.
*   **Key Features**:
    *   **Data Card**: Explains the dataset origin (Synthetic), size, and privacy protections.
    *   **Model Card**: Details the Naive Bayes classifier's version, accuracy, and known limitations.
    *   **System Policy**: The "Constitution" of the AI system.

### **Tab 5: Analytics (The "Risk Radar")**
*   **Purpose**: Macro-level view of risk exposure.
*   **What it shows**: Charts and simulations.
*   **Key Features**:
    *   **Monte Carlo Simulation**: A statistical engine that predicts financial impact (Mean Impact vs. VaR) based on current signal volatility.
    *   **Category Distribution**: Breakdown of signals by type (Fraud vs. Service vs. Info).

---

## 3. Core Code Capabilities

The backend logic is the "Brain" of the operation.

1.  **Synthetic-Only Governance**: The system is hard-coded to reject any data not flagged as `synthetic: True`. This prevents real customer data from ever touching the model during dev/test.
2.  **PII Redaction Engine**: Before any text reaches the AI, a Regex layer automatically sanitizes it.
    *   Removes **Phone Numbers** (UAE/Intl)
    *   Removes **Emails**
    *   Removes **IBANs**
    *   Removes **Social Handles** (@username)
3.  **Timestamp-Aware Clustering**: The clustering engine (Stage 3) uses the *actual timestamp of the tweet*, not the server time. This ensures that historical data replay accurately simulates past events (e.g., detecting a "spike" that happened last Tuesday).
4.  **Role-Based Access Control (RBAC)**: The API enforces strict permissions. An "Analyst" can only *recommend*, while a "Reviewer" must *approve*.
5.  **Hybrid Gating**: The system uses a "Volume Override" mechanism. Even if the AI is unsure (Low Confidence), if the *Volume* of similar messages is high, it overrides the gate and surfaces the alert (The "Viral Signal" logic).

---

## 4. Repository Guide (File-by-File)

### **`/src` (Source Code)**

| File | Purpose |
|------|---------|
| **`dashboard.py`** | **The Frontend.** Contains the Streamlit application code, UI styling (CSS), tab logic, and interactive components. |
| **`responsible_ai_pipeline.py`** | **The Orchestrator.** This is the "Main Loop" that connects all 10 stages (Loader -> Guardrails -> Classifier -> Scorer -> Gate -> Cluster -> etc.) into a single `process()` function. |
| **`guardrails.py`** | **The Bouncer.** (Stage 0 & 10). Handles PII redaction and policy checks. If data doesn't pass these checks, the pipeline stops immediately. |
| **`data_loader.py`** | **The Ingestor.** Reads the CSV file. It normalizes distinct formats into a standard `Event` object and converts strings to datetime objects. |
| **`naive_bayes_classifier.py`** | **The Brain (Stage 1).** A probabilistic model that categorizes text. It uses a "Key-Value" dictionary of weighted keywords (e.g., "OTP" = Fraud, "Down" = Service) to calculate probabilities. |
| **`clustering_engine.py`** | **The Grouper (Stage 3).** Groups individual tweets into "Clusters" or "Incidents" based on time windows and text similarity. |
| **`risk_scorer.py`** | **The Calculator.** Assigns the 0-10 score. It combines the Classifier's probability with the Cluster's velocity (speed of growth) to determine urgency. |
| **`signal_gate.py`** | **The Filter (Stage 2).** Decides what is "Noise" vs. "Signal". It manages the thresholds for passing data downstream. |
| **`authz.py`** | **The Police.** Defines the RBAC policy (Analyst vs Admin) and checks headers for permission before API actions. |
| **`simulation_engine.py`** | **The Simulator.** Contains the Monte Carlo logic used in the Analytics tab. It runs thousands of random scenarios to predict risk. |
| **`api.py`** | **The Server.** A FastAPI backend that exposes the pipeline as a REST API (useful if you wanted to connect a mobile app instead of the website). |
| **`dashboard_old.py`** | Legacy frontend file (deprecated). |

### **`/data` (Datasets & Documentation)**

| File | Purpose |
|------|---------|
| **`synthetic_social_signals_mashreq.csv`** | **The Data.** The master dataset containing 162 records of synthetic tweets used to drive the demo. |
| **`data_card.json`** | **Metadata.** A JSON file describing the dataset for compliance (part of Responsible AI). |
| **`model_card.json`** | **Metadata.** A JSON file describing the Model's performance and version history. |

### **Root Files**

| File | Purpose |
|------|---------|
| **`launcher.py`** | A utility script to start the system easily. |
| **`requirements.txt`** | List of python libraries needed (pandas, streamlit, fastapi, etc.). |

---

## 5. Setup & deployment Guide

### Prerequisites
- Python 3.9+
- Git

### Installation
1.  **Clone the repository**:
    ```bash
    git clone https://github.com/SomanaNoor/Mashreq-Controlled-Chaos.git
    cd Mashreq-Controlled-Chaos
    ```
2.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

### Running the System
You can launch the full system (Dashboard + API) using the launcher script:
```bash
python launcher.py
```
*   **Dashboard** will be available at: `http://localhost:8501`
*   **API** will be available at: `http://localhost:8000`

---

## 6. Architecture & Data Flow

1.  **Ingestion Layer**: `data_loader.py` reads raw CSV data and converts it into structured `Event` objects.
2.  **Governance Layer (Stage 0)**: `guardrails.py` regex-scrubs all PII. If `synthetic: True` is missing, data is rejected.
3.  **Intelligence Layer (Stage 1-3)**:
    *   **Classification**: Model predicts probability of Fraud/Service/Misinfo.
    *   **Gating**: Low confidence signals are dropped *unless* volume is high.
    *   **Clustering**: Related signals are grouped into "Incidents".
4.  **Presentation Layer**: The Streamlit dashboard visualizes these Incidents for human review.
5.  **Action Layer**: Human decisions (Escalate/Dismiss) are logged to the Audit Trail.

---

## 7. Technical Decisions (Why this stack?)

*   **Why Streamlit?** Rapid prototyping of data-heavy interfaces. It allows for "Python-only" frontend development, ensuring data scientists can maintain the UI.
*   **Why Naive Bayes?** In banking conformance, **Interpretability > Accuracy**. A Naive Bayes model is white-box; we can see exactly *which* keyword contributed to the fraud score. Neural networks (like BERT) are black boxes and harder to audit for regulatory compliance.
*   **Why Simulation?** Operational risk is about "Tail Events". The Monte Carlo engine allows the bank to stress-test "What if" scenarios for liquidity crises without waiting for a real one to happen.
